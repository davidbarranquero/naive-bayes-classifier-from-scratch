{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce34e22",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier - From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44acdd",
   "metadata": {},
   "source": [
    "## David Barranquero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5feef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a41fb9",
   "metadata": {},
   "source": [
    "In this task, we will implement a Naive Bayes Classifier from scratch. The Naive Bayes Classifier utilises Bayes Theorem, \n",
    "\n",
    "$$ P(Y|X_1,..., X_n) = \\frac{P(X_1,..., X_n|Y) \\times P(Y)}{P(X_1,...,X_n)}$$ for class Y, and features $X_1,...,X_n$.\n",
    "\n",
    "From this, we attempt to calculate the posterior probability of a certain class, given the data observed. However, the theorem requires use to know the joint distributions of the features, which is generally not possible with real data. If however, we multiply by the constant denominator, and naively assume each feature is conditionally independent within each class, then we can significantly reduce the computational cost, and calculate a value which is still proportional to the posterior probability, allowing us to still accurately classify data. Our equation hence becomes,\n",
    "\n",
    "$$P(Y|X_1,..., X_n) \\propto P(X_1|Y) \\times...P(X_n|Y)\\times P(Y)$$\n",
    "\n",
    "The idea is to perform this calculation for each class $y \\in Y$, and then assign the class label to the class which has the highest 'proportional' posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec20fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_no = 4760396\n",
    "np.random.seed(random_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bca082",
   "metadata": {},
   "source": [
    "The dataset is a dataset from the 1999 KDD Cup, which tasked participants with correctly classifiying network connections given a set of predictors and identifying intrusions. For this project, we will just be using the 10% dataset. The dataset and supporting documentation can be found at http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78dee447",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'duration',\n",
    "    'protocol_type',\n",
    "    'service',\n",
    "    'flag',\n",
    "    'src_bytes',\n",
    "    'dst_bytes',\n",
    "    'land',\n",
    "    'wrong_fragment',\n",
    "    'urgent',\n",
    "    'hot',\n",
    "    'num_failed_logins',\n",
    "    'logged_in',\n",
    "    'num_compromised',\n",
    "    'root_shell',\n",
    "    'su_attempted',\n",
    "    'num_root',\n",
    "    'num_file_creations',\n",
    "    'num_shells',\n",
    "    'num_access_files',\n",
    "    'num_outbound_cmds',\n",
    "    'is_host_login',\n",
    "    'is_guest_login',\n",
    "    'count',\n",
    "    'srv_count',\n",
    "    'serror_rate',\n",
    "    'srv_serror_rate',\n",
    "    'rerror_rate',\n",
    "    'srv_rerror_rate',\n",
    "    'same_srv_rate',\n",
    "    'diff_srv_rate',\n",
    "    'srv_diff_host_rate',\n",
    "    'dst_host_count',\n",
    "    'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate',\n",
    "    'attack_type'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b31834",
   "metadata": {},
   "source": [
    "We begin first by reading in the file and setting the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ed2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('NBData/kddcup.data_10_percent', header=None)\n",
    "kddcup = pd.DataFrame(file)\n",
    "kddcup.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53251cef",
   "metadata": {},
   "source": [
    "Next we display some information on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04587ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 494021 entries, 0 to 494020\n",
      "Data columns (total 42 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   duration                     494021 non-null  int64  \n",
      " 1   protocol_type                494021 non-null  object \n",
      " 2   service                      494021 non-null  object \n",
      " 3   flag                         494021 non-null  object \n",
      " 4   src_bytes                    494021 non-null  int64  \n",
      " 5   dst_bytes                    494021 non-null  int64  \n",
      " 6   land                         494021 non-null  int64  \n",
      " 7   wrong_fragment               494021 non-null  int64  \n",
      " 8   urgent                       494021 non-null  int64  \n",
      " 9   hot                          494021 non-null  int64  \n",
      " 10  num_failed_logins            494021 non-null  int64  \n",
      " 11  logged_in                    494021 non-null  int64  \n",
      " 12  num_compromised              494021 non-null  int64  \n",
      " 13  root_shell                   494021 non-null  int64  \n",
      " 14  su_attempted                 494021 non-null  int64  \n",
      " 15  num_root                     494021 non-null  int64  \n",
      " 16  num_file_creations           494021 non-null  int64  \n",
      " 17  num_shells                   494021 non-null  int64  \n",
      " 18  num_access_files             494021 non-null  int64  \n",
      " 19  num_outbound_cmds            494021 non-null  int64  \n",
      " 20  is_host_login                494021 non-null  int64  \n",
      " 21  is_guest_login               494021 non-null  int64  \n",
      " 22  count                        494021 non-null  int64  \n",
      " 23  srv_count                    494021 non-null  int64  \n",
      " 24  serror_rate                  494021 non-null  float64\n",
      " 25  srv_serror_rate              494021 non-null  float64\n",
      " 26  rerror_rate                  494021 non-null  float64\n",
      " 27  srv_rerror_rate              494021 non-null  float64\n",
      " 28  same_srv_rate                494021 non-null  float64\n",
      " 29  diff_srv_rate                494021 non-null  float64\n",
      " 30  srv_diff_host_rate           494021 non-null  float64\n",
      " 31  dst_host_count               494021 non-null  int64  \n",
      " 32  dst_host_srv_count           494021 non-null  int64  \n",
      " 33  dst_host_same_srv_rate       494021 non-null  float64\n",
      " 34  dst_host_diff_srv_rate       494021 non-null  float64\n",
      " 35  dst_host_same_src_port_rate  494021 non-null  float64\n",
      " 36  dst_host_srv_diff_host_rate  494021 non-null  float64\n",
      " 37  dst_host_serror_rate         494021 non-null  float64\n",
      " 38  dst_host_srv_serror_rate     494021 non-null  float64\n",
      " 39  dst_host_rerror_rate         494021 non-null  float64\n",
      " 40  dst_host_srv_rerror_rate     494021 non-null  float64\n",
      " 41  attack_type                  494021 non-null  object \n",
      "dtypes: float64(15), int64(23), object(4)\n",
      "memory usage: 158.3+ MB\n"
     ]
    }
   ],
   "source": [
    "kddcup.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba509fb",
   "metadata": {},
   "source": [
    "We can see that we do not have any missing values, and all datatypes are quantitative, either discrete counts or continuous, except for our target variable. Furthermore, we display the first few rows, checking our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c059cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>attack_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
       "0         0           tcp    http   SF        181       5450     0   \n",
       "1         0           tcp    http   SF        239        486     0   \n",
       "2         0           tcp    http   SF        235       1337     0   \n",
       "3         0           tcp    http   SF        219       1337     0   \n",
       "4         0           tcp    http   SF        217       2032     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0               0       0    0  ...                   9   \n",
       "1               0       0    0  ...                  19   \n",
       "2               0       0    0  ...                  29   \n",
       "3               0       0    0  ...                  39   \n",
       "4               0       0    0  ...                  49   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                     1.0                     0.0   \n",
       "1                     1.0                     0.0   \n",
       "2                     1.0                     0.0   \n",
       "3                     1.0                     0.0   \n",
       "4                     1.0                     0.0   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.11                          0.0   \n",
       "1                         0.05                          0.0   \n",
       "2                         0.03                          0.0   \n",
       "3                         0.03                          0.0   \n",
       "4                         0.02                          0.0   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                   0.0                       0.0                   0.0   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   0.0                       0.0                   0.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   dst_host_srv_rerror_rate  attack_type  \n",
       "0                       0.0      normal.  \n",
       "1                       0.0      normal.  \n",
       "2                       0.0      normal.  \n",
       "3                       0.0      normal.  \n",
       "4                       0.0      normal.  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kddcup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55529d09",
   "metadata": {},
   "source": [
    "We need to reduce the number of classes down from 23 to 5. We will hence create a mapping based on the information in the 'training_attack_types.txt' supporting file, for these 5 classes. The text can be found at http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad736109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    'back.': 'dos',\n",
    "    'buffer_overflow.': 'u2r',\n",
    "    'ftp_write.': 'r2l',\n",
    "    'guess_passwd.': 'r2l',\n",
    "    'imap.': 'r2l',\n",
    "    'ipsweep.': 'probe',\n",
    "    'land.': 'dos',\n",
    "    'loadmodule.': 'u2r',\n",
    "    'multihop.': 'r2l',\n",
    "    'neptune.': 'dos',\n",
    "    'nmap.': 'probe',\n",
    "    'normal.': 'normal',\n",
    "    'perl.': 'u2r',\n",
    "    'phf.': 'r2l',\n",
    "    'pod.': 'dos',\n",
    "    'portsweep.': 'probe',\n",
    "    'rootkit.': 'u2r',\n",
    "    'satan.': 'probe',\n",
    "    'smurf.': 'dos',\n",
    "    'spy.': 'r2l',\n",
    "    'teardrop.': 'dos',\n",
    "    'warezclient.': 'r2l',\n",
    "    'warezmaster.': 'r2l',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64ac66",
   "metadata": {},
   "source": [
    "Then we use this dictionary to map to the five classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "011020af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos       391458\n",
      "normal     97278\n",
      "probe       4107\n",
      "r2l         1126\n",
      "u2r           52\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Use a map to fill the values\n",
    "kddcup['class'] = kddcup['attack_type'].map(class_dict)\n",
    "\n",
    "# Check data worked\n",
    "print(kddcup['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d64a3",
   "metadata": {},
   "source": [
    "We can also see from this that there is a major class imbalance, which may affect the performance of our classifier. Finally, we drop the old class column from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f209ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "kddcup = kddcup.drop('attack_type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908f526",
   "metadata": {},
   "source": [
    "### Handling Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f258796",
   "metadata": {},
   "source": [
    "To handle our continuous features, we are going to use binning. We display the information on our continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ec94aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>rerror_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "      <td>494021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.176687</td>\n",
       "      <td>0.176609</td>\n",
       "      <td>0.057433</td>\n",
       "      <td>0.057719</td>\n",
       "      <td>0.791547</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.753780</td>\n",
       "      <td>0.030906</td>\n",
       "      <td>0.601935</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.176754</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.057412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.380717</td>\n",
       "      <td>0.381017</td>\n",
       "      <td>0.231623</td>\n",
       "      <td>0.232147</td>\n",
       "      <td>0.388189</td>\n",
       "      <td>0.082205</td>\n",
       "      <td>0.142397</td>\n",
       "      <td>0.410781</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.481309</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.380593</td>\n",
       "      <td>0.380919</td>\n",
       "      <td>0.230590</td>\n",
       "      <td>0.230140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         serror_rate  srv_serror_rate    rerror_rate  srv_rerror_rate  \\\n",
       "count  494021.000000    494021.000000  494021.000000    494021.000000   \n",
       "mean        0.176687         0.176609       0.057433         0.057719   \n",
       "std         0.380717         0.381017       0.231623         0.232147   \n",
       "min         0.000000         0.000000       0.000000         0.000000   \n",
       "25%         0.000000         0.000000       0.000000         0.000000   \n",
       "50%         0.000000         0.000000       0.000000         0.000000   \n",
       "75%         0.000000         0.000000       0.000000         0.000000   \n",
       "max         1.000000         1.000000       1.000000         1.000000   \n",
       "\n",
       "       same_srv_rate  diff_srv_rate  srv_diff_host_rate  \\\n",
       "count  494021.000000  494021.000000       494021.000000   \n",
       "mean        0.791547       0.020982            0.028997   \n",
       "std         0.388189       0.082205            0.142397   \n",
       "min         0.000000       0.000000            0.000000   \n",
       "25%         1.000000       0.000000            0.000000   \n",
       "50%         1.000000       0.000000            0.000000   \n",
       "75%         1.000000       0.000000            0.000000   \n",
       "max         1.000000       1.000000            1.000000   \n",
       "\n",
       "       dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "count           494021.000000           494021.000000   \n",
       "mean                 0.753780                0.030906   \n",
       "std                  0.410781                0.109259   \n",
       "min                  0.000000                0.000000   \n",
       "25%                  0.410000                0.000000   \n",
       "50%                  1.000000                0.000000   \n",
       "75%                  1.000000                0.040000   \n",
       "max                  1.000000                1.000000   \n",
       "\n",
       "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "count                494021.000000                494021.000000   \n",
       "mean                      0.601935                     0.006684   \n",
       "std                       0.481309                     0.042133   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       1.000000                     0.000000   \n",
       "75%                       1.000000                     0.000000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "count         494021.000000             494021.000000         494021.000000   \n",
       "mean               0.176754                  0.176443              0.058118   \n",
       "std                0.380593                  0.380919              0.230590   \n",
       "min                0.000000                  0.000000              0.000000   \n",
       "25%                0.000000                  0.000000              0.000000   \n",
       "50%                0.000000                  0.000000              0.000000   \n",
       "75%                0.000000                  0.000000              0.000000   \n",
       "max                1.000000                  1.000000              1.000000   \n",
       "\n",
       "       dst_host_srv_rerror_rate  \n",
       "count             494021.000000  \n",
       "mean                   0.057412  \n",
       "std                    0.230140  \n",
       "min                    0.000000  \n",
       "25%                    0.000000  \n",
       "50%                    0.000000  \n",
       "75%                    0.000000  \n",
       "max                    1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kddcup_float = kddcup.select_dtypes('float64')\n",
    "float_cols = kddcup_float.columns.tolist()\n",
    "kddcup_float.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3da796",
   "metadata": {},
   "source": [
    "From this, we can see that our continuous features are all contained within the interval of (0,1). Furthermore, we have a very non-uniform dispersion of values. We will use a simple approach and cut the continuous values into 2 equal-width bins, hence placing all values <= 0.5 in one bin, and all values > 0.5 in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d29292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our new DataFrame\n",
    "kddcup_binned = pd.DataFrame()\n",
    "\n",
    "feature_columns = list(kddcup.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55585d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to bin our continous variable\n",
    "for column in feature_columns:\n",
    "\n",
    "    if column in float_cols:\n",
    "        kddcup_binned[column] = pd.cut(kddcup[column], \n",
    "                                       bins=2, \n",
    "                                       labels=['<=0.5', '>0.5'])\n",
    "    \n",
    "    else:\n",
    "        kddcup_binned[column] = kddcup[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35d4da",
   "metadata": {},
   "source": [
    "To store our probabilities, we will use a nested dictionary with the architecture: dictionary -> class -> feature -> value -> probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c32e4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_dict = {\n",
    "    'dos': {},\n",
    "    'normal': {},\n",
    "    'probe': {},\n",
    "    'r2l': {},\n",
    "    'u2r': {}\n",
    "}\n",
    "\n",
    "# Remove the class label, on the end of the list\n",
    "feature_columns.pop(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0a247",
   "metadata": {},
   "source": [
    "### Handling Zero Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517fa67a",
   "metadata": {},
   "source": [
    "Next, we loop through the entire dataset and build the dictionary (only storing the initial small value) so that it has seen every possible value. This way, if we are unlucky enough to have unique class values in our test data, nothing breaks.\n",
    "\n",
    "This small value is a very small value we are adding to all probabilities to fix the 'Zero Count' problem. This occurs when the count of a particular feature value for a given class is 0, and hence the probability P(X1|Y) = 0. As such, our entire multiplication becomes 0. The log transformation used to fix numerical underflow (explained in more detail later), also will have issues as log(0) does not exist. As such, we use Laplace smoothing, which involves adding a value (usually 1) to all the counts, and if the counts are large enough, this resolves our issue without affecting too much the probabilities. Because we are storing probabilities and not counts, we will use the value 0.000001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a587df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every column\n",
    "for column in feature_columns:\n",
    "    \n",
    "    # For every class I have, add this column to its nested dictionary\n",
    "    for target_class in probability_dict:\n",
    "        probability_dict[target_class][column] = {}\n",
    "\n",
    "    # Then get all the possible column values\n",
    "    all_column_values = set(kddcup_binned[column].tolist())\n",
    "\n",
    "    # And for each of these\n",
    "    for value in all_column_values:\n",
    "        \n",
    "        # For every class dictionary where this value appears, \n",
    "        # Add our 'Zero Count' fix\n",
    "        for target_class in probability_dict:\n",
    "            probability_dict[target_class][column][value] = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b1018",
   "metadata": {},
   "source": [
    "### Training and Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512dd6f5",
   "metadata": {},
   "source": [
    "Now that we have initialised our dictionary, we can safely split the dataset into a training and test set, with 70% of our data in the training set, and 30% in the testing set. We will perform stratified sampling via proportional allocation, to ensure a proportional class distribution in both our training and testing set, which lowers the sampling variance, and should hopefully produce a better classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d72e89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we separate all the values of the data\n",
    "dos_idx = kddcup_binned.index[kddcup['class'] == 'dos'].tolist()\n",
    "normal_idx = kddcup_binned.index[kddcup['class'] == 'normal'].tolist()\n",
    "probe_idx = kddcup_binned.index[kddcup['class'] == 'probe'].tolist()\n",
    "r2l_idx = kddcup_binned.index[kddcup['class'] == 'r2l'].tolist()\n",
    "u2r_idx = kddcup_binned.index[kddcup['class'] == 'u2r'].tolist()\n",
    "\n",
    "# And store these in a list\n",
    "classes = [dos_idx, normal_idx, probe_idx, r2l_idx, u2r_idx]\n",
    "\n",
    "# Our function to split the records into training and test,\n",
    "def split_train_test(test_split):\n",
    "    training_idx_full = []\n",
    "    test_idx_full = []\n",
    "    \n",
    "    # We perform this random sampling for each stratum, labels of the target variable\n",
    "    for j in range(0, 5):  \n",
    "        training_idx, test_idx = shuffle_and_split(classes[j], \n",
    "                                                   len(classes[j]), \n",
    "                                                   test_split)\n",
    "        training_idx_full.extend(training_idx)\n",
    "        test_idx_full.extend(test_idx)\n",
    "\n",
    "    return kddcup_binned.iloc[training_idx_full], kddcup_binned.iloc[test_idx_full]\n",
    "\n",
    "\n",
    "# Our function to decide which rows will be for the test data set\n",
    "def shuffle_and_split(indices, length, test_ratio):\n",
    "    np.random.shuffle(indices)\n",
    "    test_set_size = round(length * test_ratio)\n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "# We call our function to get the training and test data\n",
    "training_data, test_data = split_train_test(0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f237a",
   "metadata": {},
   "source": [
    "Next, we perform checks on our training and testing datasets to ensure they were split correcty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "931ca531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proportion of data in Training set: 0.7\n",
      "Proportion of data in Test set: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks on data size\n",
    "print('\\nProportion of data in Training set:', round(len(training_data) / len(kddcup), 4))\n",
    "print('Proportion of data in Test set:', round(len(test_data) / len(kddcup), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "660d2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Training Strat  Test Strat  Original DF  Training Diff     Test Diff\n",
      "dos           0.792392    0.792390     0.792391   4.692757e-07 -1.094980e-06\n",
      "normal        0.196912    0.196908     0.196911   9.858647e-07 -2.300358e-06\n",
      "probe         0.008314    0.008313     0.008313   2.819599e-07 -6.579084e-07\n",
      "r2l           0.002279    0.002281     0.002279  -5.803212e-07  1.354087e-06\n",
      "u2r           0.000104    0.000108     0.000105  -1.156779e-06  2.699159e-06\n"
     ]
    }
   ],
   "source": [
    "# Calculating how well our Stratified Sampler performed\n",
    "training_prop = training_data['class'].value_counts() / len(training_data)\n",
    "test_prop = test_data['class'].value_counts() / len(test_data)\n",
    "original_prop = kddcup['class'].value_counts() / len(kddcup)\n",
    "\n",
    "# Build a DF containing the performance stats\n",
    "strat_stats = pd.DataFrame({\n",
    "    \"Training Strat\": training_prop,\n",
    "    \"Test Strat\": test_prop,\n",
    "    \"Original DF\": original_prop,\n",
    "    \"Training Diff\": training_prop - original_prop,\n",
    "    \"Test Diff\": test_prop - original_prop\n",
    "}).sort_index()\n",
    "\n",
    "# Display the stats\n",
    "print(strat_stats.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fcf12",
   "metadata": {},
   "source": [
    "As we can see, we have maintained our class proportions in the training and testing set. Next, we will build our probability dictionary, using the data in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd63927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we divide our training data into the classes\n",
    "class_dos = training_data[training_data['class'] == 'dos']\n",
    "class_normal = training_data[training_data['class'] == 'normal']\n",
    "class_probe = training_data[training_data['class'] == 'probe']\n",
    "class_r2l = training_data[training_data['class'] == 'r2l']\n",
    "class_u2r = training_data[training_data['class'] == 'u2r']\n",
    "\n",
    "# For every column we have\n",
    "for column in feature_columns:\n",
    "\n",
    "    # For every class we have\n",
    "    for class_df in class_dos, class_normal, class_probe, class_r2l, class_u2r:\n",
    "\n",
    "        # Get the unique counts of each feature value, given that class\n",
    "        unique_value_counts = class_df[column].value_counts()\n",
    "\n",
    "        # And their names, for the dictionary keys\n",
    "        unique_value_names = unique_value_counts.index.tolist()\n",
    "\n",
    "        # And convert the counts to probabilities\n",
    "        unique_value_probs = unique_value_counts.to_numpy() * (1/class_df.shape[0])\n",
    "\n",
    "        # Then use an if else statement to add that particular feature values probability\n",
    "        # To the previously initialised epsilon value\n",
    "        if class_df is class_dos:\n",
    "            for i in range(len(unique_value_names)):\n",
    "                probability_dict['dos'][column][unique_value_names[i]] += unique_value_probs[i]\n",
    "\n",
    "        elif class_df is class_normal:\n",
    "            for i in range(len(unique_value_names)):\n",
    "                probability_dict['normal'][column][unique_value_names[i]] += unique_value_probs[i]\n",
    "\n",
    "        elif class_df is class_probe:\n",
    "            for i in range(len(unique_value_names)):\n",
    "                probability_dict['probe'][column][unique_value_names[i]] += unique_value_probs[i]\n",
    "\n",
    "        elif class_df is class_r2l:\n",
    "            for i in range(len(unique_value_names)):\n",
    "                probability_dict['r2l'][column][unique_value_names[i]] += unique_value_probs[i]\n",
    "\n",
    "        else:\n",
    "            for i in range(len(unique_value_names)):\n",
    "                probability_dict['u2r'][column][unique_value_names[i]] += unique_value_probs[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ce04e",
   "metadata": {},
   "source": [
    "Before we begin testing, we will need to calculate our prior class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06335778",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_prob_dos = class_dos.shape[0] / training_data.shape[0]\n",
    "prior_prob_normal = class_normal.shape[0] / training_data.shape[0]\n",
    "prior_prob_probe = class_probe.shape[0] / training_data.shape[0]\n",
    "prior_prob_r2l = class_r2l.shape[0] / training_data.shape[0]\n",
    "prior_prob_u2r = class_u2r.shape[0] / training_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65986863",
   "metadata": {},
   "source": [
    "### Handling Numerical Underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621251c",
   "metadata": {},
   "source": [
    "Now we can begin testing. When calculating the proportional posterior probability, we are going to address another issue that can occur with the Naive Bayes classifier, numerical underflow. Because we are multiplying many small values together which are less than 0, their product will get smaller and smaller. This can lead to the final value being rounded to 0 by the program, even though the true value is not 0. Hence, we cannot do comparison between posterior probabilities. To resolve this issue we can perform a log transformation of our values, as log(A x B) = log(A) + log(B), which will avoid accidentally rounding to 0. Furthermore, log transformations preserve the proportionality of the equation, meaning the A value and B value that maximise A x B will also maximise log(A) + log(B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6ab81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store our predictions in a list, to be used for the confusion matrix later\n",
    "pred_list = []\n",
    "\n",
    "# For every training record we have\n",
    "for idx in range(0, test_data.shape[0]):\n",
    "\n",
    "    # Retrieve the row\n",
    "    test_record = test_data.iloc[idx]\n",
    "\n",
    "    # We initialise the probabilities to 0\n",
    "    dos_prob = 0\n",
    "    normal_prob = 0\n",
    "    probe_prob = 0\n",
    "    r2l_prob = 0\n",
    "    u2r_prob = 0\n",
    "\n",
    "    # For every feature column we have for this test record\n",
    "    for i in range(len(feature_columns)):\n",
    "        \n",
    "        # Add the log of the conditional probability to the probability\n",
    "        dos_prob += log(probability_dict['dos'][columns[i]][test_record.iloc[i]])\n",
    "        \n",
    "        normal_prob += log(probability_dict['normal'][columns[i]][test_record.iloc[i]])\n",
    "        \n",
    "        probe_prob += log(probability_dict['probe'][columns[i]][test_record.iloc[i]])\n",
    "        \n",
    "        r2l_prob += log(probability_dict['r2l'][columns[i]][test_record.iloc[i]])\n",
    "        \n",
    "        u2r_prob += log(probability_dict['u2r'][columns[i]][test_record.iloc[i]])\n",
    "\n",
    "    # Then add the log of the prior probability\n",
    "    dos_prob += log(prior_prob_dos)\n",
    "    normal_prob += log(prior_prob_normal)\n",
    "    probe_prob += log(prior_prob_probe)\n",
    "    r2l_prob += log(prior_prob_r2l)\n",
    "    u2r_prob += log(prior_prob_u2r)\n",
    "\n",
    "    # Work out which probability if the max\n",
    "    max_prob = max(dos_prob, normal_prob, probe_prob, r2l_prob, u2r_prob)\n",
    "\n",
    "    # Whichever class maximised the proportional posterior probability, \n",
    "    # Is the class we predict.\n",
    "    if max_prob == dos_prob:\n",
    "        pred_list.append([test_record.name, 'dos'])\n",
    "    elif max_prob == normal_prob:\n",
    "        pred_list.append([test_record.name, 'normal'])\n",
    "    elif max_prob == probe_prob:\n",
    "        pred_list.append([test_record.name, 'probe'])\n",
    "    elif max_prob == r2l_prob:\n",
    "        pred_list.append([test_record.name, 'r2l'])\n",
    "    elif max_prob == u2r_prob:\n",
    "        pred_list.append([test_record.name, 'u2r'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee37c6d",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460a1e2",
   "metadata": {},
   "source": [
    "We will now manually construct a confusion matrix to display the classification results and assess our Naive Bayes Classifier's performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4add744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a DataFrame from the predictions, attaching the row index (for joining)\n",
    "pred_df = pd.DataFrame(pred_list, columns=['Row Index', 'Prediction'])\n",
    "\n",
    "# And get the true class labels of our values\n",
    "actual_df = test_data['class'].copy()\n",
    "\n",
    "# Then we construct new dataframes, merging our pred & actual label for each record\n",
    "performance_df = pd.merge(pred_df, actual_df, left_on='Row Index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54b92c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function to construct a row of the confusion matrix\n",
    "def confusion_matrix_row(label, DF, final_row):\n",
    "    class_labels = ['dos', 'normal', 'probe', 'r2l', 'u2r']\n",
    "\n",
    "    # Get the prediction counts for this class\n",
    "    pred_counts = DF[DF['class'] == label]['Prediction'].value_counts()\n",
    "    pred_counts_list = []\n",
    "    total = 0\n",
    "    sensitivity = 0\n",
    "    \n",
    "    # For every label in the class labels\n",
    "    for j in range(len(class_labels)):  \n",
    "        \n",
    "        # If this label was predicted\n",
    "        if class_labels[j] in pred_counts.index:  \n",
    "            \n",
    "            # Add the number of predictions to the list\n",
    "            pred_counts_list.append(pred_counts.loc[class_labels[j]])  \n",
    "            \n",
    "            # Increment the counter\n",
    "            total += pred_counts.loc[class_labels[j]]  \n",
    "            \n",
    "            # If this is on the diagonal, update the sensitivity\n",
    "            if class_labels[j] == label:  \n",
    "                sensitivity += pred_counts.loc[class_labels[j]]  \n",
    "                \n",
    "            # And the counter of the final row\n",
    "            final_row[class_labels[j]] += pred_counts.loc[class_labels[j]]  \n",
    "            \n",
    "        # If we didn't see this class, we didn't label a test record as it, \n",
    "        # So assign a 0 to the row \n",
    "        else:  \n",
    "            pred_counts_list.append(0)\n",
    "            \n",
    "    # Add the total of the true class size and the sensitivity        \n",
    "    pred_counts_list.append(total)  \n",
    "\n",
    "    # If the total is 0, just add 0, if not, add the sensitivity\n",
    "    if total == 0:\n",
    "        pred_counts_list.append(0)\n",
    "    else:\n",
    "        pred_counts_list.append(100 * sensitivity / total)\n",
    "    return pred_counts_list, final_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cc7aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function to construct the confusion matrices\n",
    "def construct_confusion_matrix(dataframe):\n",
    "    column_labels = ['dos', \n",
    "                     'normal', \n",
    "                     'probe', \n",
    "                     'r2l', \n",
    "                     'u2r', \n",
    "                     'Total', \n",
    "                     'Sensitivity']\n",
    "    \n",
    "    row_labels = ['dos', \n",
    "                  'normal', \n",
    "                  'probe', \n",
    "                  'r2l', \n",
    "                  'u2r', \n",
    "                  'Total']\n",
    "\n",
    "    final_matrix_row = {\n",
    "        'dos': 0,\n",
    "        'normal': 0,\n",
    "        'probe': 0,\n",
    "        'r2l': 0,\n",
    "        'u2r': 0\n",
    "    }\n",
    "\n",
    "    # We build one row a time, for all 'actual' classes, what were they predicted as\n",
    "    row_dos, final_matrix_row = confusion_matrix_row('dos', \n",
    "                                                     dataframe, \n",
    "                                                     final_matrix_row)\n",
    "    \n",
    "    row_normal, final_matrix_row = confusion_matrix_row('normal', \n",
    "                                                        dataframe, \n",
    "                                                        final_matrix_row)\n",
    "    \n",
    "    row_probe, final_matrix_row = confusion_matrix_row('probe', \n",
    "                                                       dataframe, \n",
    "                                                       final_matrix_row)\n",
    "    \n",
    "    row_r2l, final_matrix_row = confusion_matrix_row('r2l', \n",
    "                                                     dataframe, \n",
    "                                                     final_matrix_row)\n",
    "    \n",
    "    row_u2r, final_matrix_row = confusion_matrix_row('u2r', \n",
    "                                                     dataframe, \n",
    "                                                     final_matrix_row)\n",
    "\n",
    "    # We then construct the final row which contains how many of this class were labelled as such\n",
    "    # And also calculate the accuracy\n",
    "    total = sum(final_matrix_row.values())\n",
    "    final_matrix_row['total'] = total\n",
    "    final_matrix_row['sensitivity'] = (row_dos[0] +\n",
    "                                       row_normal[1] +\n",
    "                                       row_probe[2] +\n",
    "                                       row_r2l[3] +\n",
    "                                       row_u2r[4]) * 100 / total\n",
    "\n",
    "    final_row = final_matrix_row.values()\n",
    "\n",
    "    # We now construct our confusion matrix as a dataframe\n",
    "    conf_matrix = pd.DataFrame(\n",
    "        [row_dos,\n",
    "         row_normal,\n",
    "         row_probe,\n",
    "         row_r2l,\n",
    "         row_u2r,\n",
    "         final_row],\n",
    "        index=row_labels,\n",
    "        columns=column_labels)\n",
    "\n",
    "    # And return our matrix and accuracy\n",
    "    accuracy = final_matrix_row['sensitivity']\n",
    "\n",
    "    return conf_matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00040e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Confusion Matrix\n",
      "\n",
      "Actual/Predicted\n",
      "           dos  normal  probe  r2l  u2r   Total  Sensitivity\n",
      "dos     113984     252   3201    0    0  117437    97.059700\n",
      "normal       1   28046   1110   24    2   29183    96.103896\n",
      "probe        3      29   1200    0    0    1232    97.402597\n",
      "r2l          3       7      0  328    0     338    97.041420\n",
      "u2r          0       4      0    1   11      16    68.750000\n",
      "Total   113991   28338   5511  353   13  148206    96.871247\n",
      "\n",
      "Naive Bayes Test Accuracy: 96.87124677813314\n"
     ]
    }
   ],
   "source": [
    "# We construct a confusion matrix to visualise the models performance.\n",
    "confusion_matrix, test_accuracy = construct_confusion_matrix(performance_df)\n",
    "print('\\nNaive Bayes Confusion Matrix')\n",
    "print('\\nActual/Predicted')\n",
    "print(confusion_matrix.head(6))\n",
    "print('\\nNaive Bayes Test Accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac76c40",
   "metadata": {},
   "source": [
    "Overall, we achieved a test accuracy of roughly 97% which is an extremely strong classification performance. It is unsurprising that our 'dos' class had the highest sensitivity, and our 'u2r' class had our lowest sensitivity, given that 'dos' constituted roughly 80% of the dataset, and 'u2r' only 0.0001% of our dataset. Given this, 69% sensitivity is actually pretty good. In summary, our Naive Bayes classifier has performed remarkably well and achieved a very strong test accuracy, which makes it a very good classifier for classifying network connections, and identifying malicious connections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
